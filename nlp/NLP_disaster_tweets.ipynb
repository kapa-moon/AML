{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data points: 7613\n",
      "Number of test data points: 3263\n",
      "\n",
      "Percentage of training tweets that are real disasters: 42.97%\n",
      "Percentage of training tweets that are not real disasters: 57.03%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "test_data = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "# (1)\n",
    "training_dp = len(train_data)\n",
    "test_dp = len(test_data)\n",
    "\n",
    "print(f\"Number of training data points: {training_dp}\")\n",
    "print(f\"Number of test data points: {test_dp}\")\n",
    "\n",
    "# (2)\n",
    "disaster_percentage = train_data['target'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nPercentage of training tweets that are real disasters: {disaster_percentage[1]:.2f}%\")\n",
    "print(f\"Percentage of training tweets that are not real disasters: {disaster_percentage[0]:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the description on Kaggle, there are 7503 *unique values* in the Text column in the training dataset. There are 4342 with target value of 0 (not real disasters) and 3271 with target value of 1 (real disasters); there are 3243 *unique values* in the Text column in the test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question (b) Split the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set: 5329\n",
      "Development Set: 2284\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 70% train, 30% dev\n",
    "train_set, dev_set = train_test_split(train_data, test_size=0.3, random_state=42)\n",
    "\n",
    "print(\"Training Set:\", len(train_set))\n",
    "print(\"Development Set:\", len(dev_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question (c) Preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yuewenyyy/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/yuewenyyy/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yuewenyyy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Preprocessed Training Data:\n",
      "1186    ash 2015 australiaûªs collapse trent bridge am...\n",
      "4071    great michigan technique camp b1g thanks goblu...\n",
      "5461    cnn tennessee movie theater shooting suspect k...\n",
      "5787                 still rioting couple hour left class\n",
      "7445    crack path wiped morning beach run surface wou...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()  # Convert to lowercase: case does not affect the meaning of the word; this helps reduce the size of the vocabulary and avoid unnecessary duplication\n",
    "\n",
    "    # Remove @ mentions and URLs: so that we can reduce unnecessary noise in the data\n",
    "    text = re.sub(r'@\\w+|http\\S+', '', text)\n",
    "\n",
    "    # Remove punctuation, except for ! and ?: punctuation does not carry much meaning, but for classifying disaster, strong punctuation like ! and ? might be useful \n",
    "    text = re.sub(r'[^\\w\\s!?]', '', text)\n",
    "\n",
    "    # Lemmatize: reduces the words to their base form; root carries meaning of the words\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "\n",
    "    # Remove stop words: they usually do not carry much meaning\n",
    "    filtered_text = ' '.join([word for word in lemmatized_words if word not in stop_words])\n",
    "    \n",
    "    # TODO: Remove numbers: numerical lexical chunks do not carry much meaning\n",
    "\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "train_set['text'] = train_set['text'].apply(preprocess)\n",
    "dev_set['text'] = dev_set['text'].apply(preprocess)\n",
    "\n",
    "print(\"\\nSample Preprocessed Training Data:\")\n",
    "print(train_set['text'].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features in the Bag of Words model: 509\n",
      "\n",
      "Sample feature names: ['apocalypse' 'area' 'armageddon' 'army' 'around' 'arson' 'atomic'\n",
      " 'attack' 'attacked' 'away']\n",
      "\n",
      "Sample feature vectors (binary Bag of Words):\n",
      "[[0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# using the CountVectorizer class in sklearn.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# let's say the words we are interested in are the ones that appear in at least 5 tweets\n",
    "#TODO: come back to see if tuning M will change the results\n",
    "M = 5 \n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, min_df=M) #binary = True, min_df=M\n",
    "\n",
    "# fit CountVectorizer only once on the training set\n",
    "vectorizer.fit(train_set['text'])\n",
    "\n",
    "# use the same instance to process training and development sets\n",
    "X_train = vectorizer.transform(train_set['text'])\n",
    "X_dev = vectorizer.transform(dev_set['text'])\n",
    "\n",
    "\n",
    "num_features = len(vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "print(f\"Total number of features in the Bag of Words model: {num_features}\")\n",
    "\n",
    "\n",
    "print(\"\\nSample feature names:\", vectorizer.get_feature_names_out()[20:30])\n",
    "\n",
    "\n",
    "print(\"\\nSample feature vectors (binary Bag of Words):\")\n",
    "print(X_train[:3].toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did I decide on M?\n",
    "\n",
    "M = 1: 11785\n",
    "M = 2: 4598\n",
    "M = 3: 3076\n",
    "M = 4: 2359\n",
    "M = 5: 1925\n",
    "M = 6: 1630\n",
    "M = 7: 1442\n",
    "M = 8: 1281\n",
    "\n",
    "The decrement in number slowed down a lot when M is above 5. Tha probably indicates that when M=5 we discarded words that only appear in few number of tweets, which might cause extra noises, and we still retain the data that are informative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Logistic regression model without regularization terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Logistic Regression without regularization (Training Set): 0.9558\n",
      "F1 Score for Logistic Regression without regularization (Development Set): 0.6836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Fit a Logistic Regression model *without* regularization \n",
    "no_reg = LogisticRegression(penalty=None, solver='lbfgs', max_iter=1000)\n",
    "\n",
    "no_reg.fit(X_train, train_set['target'])\n",
    "\n",
    "# on train set\n",
    "train_preds_no_reg = no_reg.predict(X_train)\n",
    "f1_train_no_reg = f1_score(train_set['target'], train_preds_no_reg)\n",
    "\n",
    "# on dev set\n",
    "dev_preds_no_reg = no_reg.predict(X_dev)\n",
    "f1_dev_no_reg = f1_score(dev_set['target'], dev_preds_no_reg)\n",
    "\n",
    "\n",
    "print(f\"F1 Score for Logistic Regression without regularization (Training Set): {f1_train_no_reg:.4f}\")\n",
    "print(f\"F1 Score for Logistic Regression without regularization (Development Set): {f1_dev_no_reg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a high discrepancy between the training and development set performance, which is an indicator of overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii. Train a logistic regression model with L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Logistic Regression with L1 regularization (Training Set): 0.8387\n",
      "F1 Score for Logistic Regression with L1 regularization (Development Set): 0.7389\n"
     ]
    }
   ],
   "source": [
    "# L1 regularization (penalty='l1')\n",
    "l1_reg = LogisticRegression(penalty='l1', solver='liblinear', max_iter=1000)\n",
    "\n",
    "# fit on the training data\n",
    "l1_reg.fit(X_train, train_set['target'])\n",
    "\n",
    "# predicts\n",
    "train_preds_l1 = l1_reg.predict(X_train)\n",
    "dev_preds_l1 = l1_reg.predict(X_dev)\n",
    "\n",
    "# eval\n",
    "f1_train_l1 = f1_score(train_set['target'], train_preds_l1)\n",
    "f1_dev_l1 = f1_score(dev_set['target'], dev_preds_l1)\n",
    "\n",
    "print(f\"F1 Score for Logistic Regression with L1 regularization (Training Set): {f1_train_l1:.4f}\")\n",
    "print(f\"F1 Score for Logistic Regression with L1 regularization (Development Set): {f1_dev_l1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Also tried to adapt the code on sklearn's documentation\n",
    "# import numpy as np\n",
    "# from sklearn import linear_model\n",
    "# from sklearn.svm import l1_min_c\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# scaler = StandardScaler(with_mean=False)\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# y_train = train_set['target']\n",
    "\n",
    "\n",
    "# cs = l1_min_c(X_train_scaled, y_train, loss=\"log\") * np.logspace(0, 10, 16)\n",
    "\n",
    "# clf = linear_model.LogisticRegression(\n",
    "#     penalty=\"l1\",\n",
    "#     solver=\"liblinear\",\n",
    "#     tol=1e-6,\n",
    "#     max_iter=1000,\n",
    "#     warm_start=True,\n",
    "#     intercept_scaling=10000.0,\n",
    "# )\n",
    "\n",
    "# coefs_ = []\n",
    "\n",
    "# for c in cs:\n",
    "#     clf.set_params(C=c)\n",
    "#     clf.fit(X_train_scaled, y_train)\n",
    "#     coefs_.append(clf.coef_.ravel().copy())\n",
    "\n",
    "# coefs_ = np.array(coefs_)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(np.log10(cs), coefs_, marker=\"o\")\n",
    "# plt.xlabel(\"log(C)\")\n",
    "# plt.ylabel(\"Coefficients\")\n",
    "# plt.title(\"L1 Regularization Path for Logistic Regression\")\n",
    "# plt.axis(\"tight\")\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iii. Train a logistic regression model with L2 regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Logistic Regression with L2 regularization (Training Set): 0.8622\n",
      "F1 Score for Logistic Regression with L2 regularization (Development Set): 0.7464\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Fit a Logistic Regression model with L2 regularization\n",
    "l2_reg = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "\n",
    "l2_reg.fit(X_train, train_set['target'])\n",
    "\n",
    "# train set\n",
    "train_preds_l2 = l2_reg.predict(X_train)\n",
    "f1_train_l2 = f1_score(train_set['target'], train_preds_l2)\n",
    "\n",
    "# dev set\n",
    "dev_preds_l2 = l2_reg.predict(X_dev)\n",
    "f1_dev_l2 = f1_score(dev_set['target'], dev_preds_l2)\n",
    "\n",
    "print(f\"F1 Score for Logistic Regression with L2 regularization (Training Set): {f1_train_l2:.4f}\")\n",
    "print(f\"F1 Score for Logistic Regression with L2 regularization (Development Set): {f1_dev_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iv. Model comparison\n",
    "\n",
    "Overfitting was observed in the model without regularization, as shown by the large gap between F1 scores on the training and development set (0.9558 vs. 0.6836). Compared to the model without regularization, in both L1 and L2 the F1 score on the training set decreased, while the development set score improved. This suggests regularization helped reduce overfitting. The L2-regularized model performed the best. It has the best F1 scores, especially on the development set, with an F1 score of 0.7464, which is the highest among the three. This indicates that it reduces overfitting problem and it has the best generalization among the three classifiers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v. Inspect the weight vector of the classifier with L1 regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the weight vector \n",
    "weights_l1 = l1_reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The top important words for deciding if a tweet is about a real disaster include the following ones:\n",
      "('mh370', np.float64(3.4391886970023005))\n",
      "('spill', np.float64(3.341914631185037))\n",
      "('derailment', np.float64(3.321095055419975))\n",
      "('airport', np.float64(3.279606218815375))\n",
      "('hiroshima', np.float64(3.277345781406181))\n",
      "('migrant', np.float64(3.1763880556007984))\n",
      "('typhoon', np.float64(3.159369334750422))\n",
      "('wildfire', np.float64(3.0648154558641068))\n",
      "('earthquake', np.float64(2.8549198981316484))\n",
      "('crew', np.float64(2.6314179188404925))\n",
      "('debris', np.float64(2.595266384043409))\n",
      "('outbreak', np.float64(2.5472103687207555))\n",
      "('drought', np.float64(2.5330847828009952))\n",
      "('evacuated', np.float64(2.514332995998605))\n",
      "('sinkhole', np.float64(2.510076190145047))\n"
     ]
    }
   ],
   "source": [
    "# investigate the important words\n",
    "\n",
    "words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# sort by weight\n",
    "feature_weights = list(zip(words, weights_l1))\n",
    "\n",
    "important_words = sorted(feature_weights, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(\"\\nThe top important words for deciding if a tweet is about a real disaster include the following ones:\")\n",
    "for word in important_words[:15]:\n",
    "    print(f\"{word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) Bernoulli Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_train = train_set['target'].values\n",
    "y_dev = dev_set['target'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X_train_processed = X_train.toarray()\n",
    "y_train = train_set['target'].values\n",
    "X_dev_processed = X_dev.toarray()\n",
    "y_dev = dev_set['target'].values\n",
    "\n",
    "\n",
    "n = X_train_processed.shape[0] \n",
    "d = X_train_processed.shape[1] \n",
    "K = 2  #binary classes\n",
    "\n",
    "# laplace smoothing parameter\n",
    "alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (1925,) into shape (1255,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[1;32m      9\u001b[0m     X_k \u001b[38;5;241m=\u001b[39m X_train_processed[y_train \u001b[38;5;241m==\u001b[39m k]\n\u001b[0;32m---> 10\u001b[0m     \u001b[43mpsis\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m (np\u001b[38;5;241m.\u001b[39msum(X_k, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m alpha) \u001b[38;5;241m/\u001b[39m (X_k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m alpha)\n\u001b[1;32m     11\u001b[0m     phis[k] \u001b[38;5;241m=\u001b[39m X_k\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(n)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrior probabilities:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (1925,) into shape (1255,)"
     ]
    }
   ],
   "source": [
    "# [ADAPTED FROM THE CLASS EXAMPLE]\n",
    "\n",
    "# shapes of the parameters\n",
    "psis = np.zeros([K, d])  \n",
    "phis = np.zeros([K])     \n",
    "\n",
    "# compute parameters \n",
    "for k in range(K):\n",
    "    X_k = X_train_processed[y_train == k]\n",
    "    psis[k] = (np.sum(X_k, axis=0) + alpha) / (X_k.shape[0] + 2 * alpha)\n",
    "    phis[k] = X_k.shape[0] / float(n)\n",
    "\n",
    "print(\"Prior probabilities:\")\n",
    "print(phis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 0.8411\n",
      "F1 Score for Bernoulli Naive Bayes on the development set is: 0.7396\n"
     ]
    }
   ],
   "source": [
    "# compute predictions using Bayes’ rule. [ADAPTED FROM THE CLASS EXAMPLE]\n",
    "\n",
    "def nb_predictions(x, psis, phis):\n",
    "    # adjust shapes\n",
    "    n, d = x.shape\n",
    "    x = np.reshape(x, (1, n, d))\n",
    "    psis = np.reshape(psis, (K, 1, d))\n",
    "    \n",
    "    # clip probabilities to avoid log(0)\n",
    "    psis = psis.clip(1e-14, 1 - 1e-14)\n",
    "    \n",
    "    # compute log-probabilities\n",
    "    logpy = np.log(phis).reshape([K, 1])\n",
    "    logpxy = x * np.log(psis) + (1 - x) * np.log(1 - psis)\n",
    "    logpyx = logpxy.sum(axis=2) + logpy\n",
    "    \n",
    "    return logpyx.argmax(axis=0).flatten(), logpyx.reshape([K, n])\n",
    "\n",
    "# training set\n",
    "train_preds, _ = nb_predictions(X_train_processed, psis, phis)\n",
    "train_accuracy = (train_preds == y_train).mean()\n",
    "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# dev set\n",
    "dev_preds, _ = nb_predictions(X_dev_processed, psis, phis)\n",
    "\n",
    "# Calculate F1 Score for development set\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_dev = f1_score(y_dev, dev_preds)\n",
    "print(f\"F1 Score for Bernoulli Naive Bayes on the development set is: {f1_dev:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Model comparison\n",
    "\n",
    "Logistic Regression with L2 Regularization has the highest F1 score on dev set which is 0.746. That is slightly better than the F1 score the Bernoulli Naive Bayes got, which is 0.7396. \n",
    "\n",
    "Generative models like Naive Bayes is less prone to overfitting (didn't need techniques like regularization). However, we know that there's a strong assumption on the conditional independence, which is often not true in real-world cases for text classfication. This might influence the performance, especially when there are comlex relationships between features (language task could be one example). \n",
    "\n",
    "Discriminative Model like Logistic Regression doesn't assume the conditional independence between features, which may allow it to capture more nuances. However, since it's more prone to overfitting, and hence it is necessary sometimes to use regularization such as L1 and L2 - this may increase the computational complexity a lot. Usually it also requires a larger dataset to be effective since it cannot leverage the priors.\n",
    "\n",
    "\n",
    "Naive Bayes assumes that words are uncorrelated and then uses Baye's theorum for determining the posterior probability, whereas Logistic Regression without necessarily assuming the distribution of the features.\n",
    "\n",
    "Words are correlated in reality, but Naive Bayes still gives reasonable accuracy in terms of this binary classification task. It could miss some nuanced relationships between words, which might make it not the most optimal model for this type of task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) N-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of 2-grams in the vocabulary (M=2): 3542\n",
      "\n",
      "Sample 10 2-grams from the vocabulary:\n",
      "0104 utc\n",
      "010401 utc20150805\n",
      "10 year\n",
      "10 yr\n",
      "101 cook\n",
      "1030 pm\n",
      "10401 utc\n",
      "109 sn\n",
      "10km maximum\n",
      "10th death\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# construct feature representations \n",
    "\n",
    "M = 2 #has to appear in at least M tweets\n",
    "# M = 5\n",
    "# 2 gram, set ngram_range=(2,2) \n",
    "vectorizer_2gram = CountVectorizer(ngram_range=(2, 2), binary=True, min_df=M)\n",
    "\n",
    "\n",
    "X_train_2gram = vectorizer_2gram.fit_transform(train_set['text'])\n",
    "\n",
    "\n",
    "X_dev_2gram = vectorizer_2gram.transform(dev_set['text'])\n",
    "\n",
    "\n",
    "num_2grams = len(vectorizer_2gram.get_feature_names_out())\n",
    "\n",
    "# the total number of 2-grams\n",
    "print(f\"Total number of 2-grams in the vocabulary (M={M}): {num_2grams}\")\n",
    "\n",
    "\n",
    "sample_2grams = vectorizer_2gram.get_feature_names_out()[:10]\n",
    "print(\"\\nSample 10 2-grams from the vocabulary:\")\n",
    "for ngram in sample_2grams:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To decide on M, similar to Bag of Words model, when M is around 4 and 5, the decrement of number valid features (i.e. appeared in M tweets) decreased and stabelized. To keep the features still informative and not noisy, I chose M = 4.\n",
    "\n",
    "(M=1): 33231\n",
    "\n",
    "(M=2): 3542\n",
    "\n",
    "(M=3): 1542\n",
    "\n",
    "(M=4): 954\n",
    "\n",
    "(M=5): 654\n",
    "\n",
    "(M=6): 483"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Logistic Regression with L2 regularization (Training Set): 0.7250\n",
      "F1 Score for Logistic Regression with L2 regularization (Dev Set): 0.5645\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression with L2 regularization\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "l2_reg_2gram = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "# Fit the model on the 2-gram training data\n",
    "l2_reg_2gram.fit(X_train_2gram, train_set['target'])\n",
    "\n",
    "# Make predictions on the training and development sets\n",
    "train_preds_2gram_l2 = l2_reg_2gram.predict(X_train_2gram)\n",
    "dev_preds_2gram_l2 = l2_reg_2gram.predict(X_dev_2gram)\n",
    "\n",
    "# Calculate F1 scores for training and development sets\n",
    "f1_train_2gram_l2 = f1_score(train_set['target'], train_preds_2gram_l2)\n",
    "f1_dev_2gram_l2 = f1_score(dev_set['target'], dev_preds_2gram_l2)\n",
    "\n",
    "print(f\"F1 Score for Logistic Regression with L2 regularization (Training Set): {f1_train_2gram_l2:.4f}\")\n",
    "print(f\"F1 Score for Logistic Regression with L2 regularization (Dev Set): {f1_dev_2gram_l2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "These results dropped significantly compared with bag of words.\n",
    "\n",
    "F1 Score for Logistic Regression with L2 regularization (Training Set): 0.5556\n",
    "\n",
    "F1 Score for Logistic Regression with L2 regularization (Dev Set): 0.4776\n",
    "\n",
    "Above are the data when M=5. When I changed M to 2, the performance went up to 0.7250 and 0.5645 (dev set).\n",
    "\n",
    "The dimentionality of the features increased a lot and also the sparsity of the model. Since the tweets are relatively short chunks, not like normal writing in long paragraphs, 2-grams are not observed as often in other tweets.\n",
    "\n",
    "Next I am gonna try Bernoulli classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "X_train_2gram_processed = X_train_2gram.toarray()\n",
    "X_train_2gram_processed = X_dev_2gram.toarray()\n",
    "y_train = train_set['target'].values\n",
    "y_dev = dev_set['target'].values\n",
    "\n",
    "# checking dimensions (TDOO: this throws an error previously)\n",
    "print(\"Shape of X_train_2gram_dense:\", X_train_2gram_processed.shape)\n",
    "print(\"Length of y_train:\", len(y_train))\n",
    "\n",
    "# ADAPTED FROM THE CLASS EXAMPLE\n",
    "def nb_predictions(x, psis, phis):\n",
    "\n",
    "    n, d = x.shape  \n",
    "    psis = psis.clip(1e-14, 1 - 1e-14)  # TODO:solve numerical issues log(0)\n",
    "    \n",
    "    logpy = np.log(phis).reshape(-1, 1) \n",
    "    logpxy = x @ np.log(psis.T) + (1 - x) @ np.log(1 - psis.T)  \n",
    "    logpyx = logpy.T + logpxy  \n",
    "\n",
    "\n",
    "    return logpyx.argmax(axis=1), logpyx\n",
    "\n",
    "\n",
    "X_train_2gram_processed = X_dev_2gram.toarray()\n",
    "dev_preds_2gram_nb, _ = nb_predictions(X_train_2gram_processed, psis, phis)\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "f1_dev_2gram_nb = f1_score(y_dev, dev_preds_2gram_nb)\n",
    "print(f\"F1 score of Bernoulli Naive Bayes on 2-gram (dev set):\", f1_dev_2gram_nb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score of Bernoulli Naive Bayes on 2-gram (dev set): 0.4884437596302003, which is very low compared with previous methods. I tried mixed N-grams, (mixing uni gram and 2 gram) and increased M to 10, but the F1 score was only improved to 0.6950617283950618."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) Determine performance with the *test set*. I choose Bag of Words with L2 regularization of Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 2795)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "train_text = train_data['text'].tolist()\n",
    "train_labels = train_data['target'].values\n",
    "vectorizer = CountVectorizer(binary=True, min_df=5)\n",
    "\n",
    "X_new = vectorizer.fit_transform(train_text)\n",
    "\n",
    "# check\n",
    "print(X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score for Logistic Regression with L2 regularization (new): 0.8758\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model_l2_reg = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "model_l2_reg.fit(X_new, train_labels)\n",
    "\n",
    "train_preds_combined = model_l2_reg.predict(X_new)\n",
    "f1_train_new= f1_score(train_labels, train_preds_combined)\n",
    "print(f\"F1 Score for Logistic Regression with L2 regularization (new): {f1_train_new:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "test_preds = model_l2_reg.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'target': test_preds\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of GenAI: debugging, search for methods, and conservative use of autocompletions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
